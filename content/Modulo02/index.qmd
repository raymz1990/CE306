---
title: "Álgebra Matricial "
code-fold: false
---

A técnica escolhida para ilustrar os conceitos de funções e derivadas em ciência de dados foi o modelo de regressão linear simples. Nós vimos na Seção 1.2.9 que neste modelo o treinamento, ou estimação dos parâmetros, se resumiu a encontrar a solução de um sistema de duas equações lineares. Agora imagine uma situação prática com milhares de potenciais variáveis explicativas: como seria o treinamento deste modelo? Assim, fica claro que precisamos de ferramentas que nos permitam lidar com muitas equações simultâneas e de preferência de uma forma compacta e eficiente. É neste contexto que as ferramentas de Álgebra Matricial são valiosas em ciência de dados. 
 
O objetivo deste Capítulo é apresentar as principais ferramentas de **Álgebra Matricial** úteis em ciência de dados. Como exemplo ilustrativo será apresentado o modelo de regressão linear múltipla que estende o modelo de regressão linear simples permitindo um número arbitrário de variáveis explicativas. Para a sua representação utilizaremos uma notação matricial em que o modelo é escrito matematicamente de uma forma compacta e de fácil entendimento. Além disso, as ferramentas de Álgebra Matricial vão permitir obter equações compactas para os estimadores dos coeficientes de regressão, o que facilita o processo de treinamento do modelo. 
 
Por fim, nós vamos discutir algumas decomposições matriciais que consistem em escrever a matriz gerada pelos dados em formas convenientes para o processo de treinamento e discutir como utilizar tais decomposições em ciência de dados. Especial ênfase será dada ao caso de problemas em que o número de variáveis explicativas é maior que o número de observações ( *High Dimensional data *). Nestas situações a decomposição em valores singulares leva ao modelo de regressão  *ridge *, também bastante popular em ciência de dados e uma extensão natural do modelo de regressão linear múltipla. 

## Vetores e escalares 

::: {.callout-important appearance="simple"}

## **Definição 2.1**

Um vetor é uma lista de $n$ números (elementos ou componentes) escritos em linha ou coluna.
:::

Em termos de notação nós vamos usar letras minúsculas em negrito, por exemplo, 

$$
\mathbf{a} = \begin{pmatrix}
a_1 & \ldots & a_n
\end{pmatrix} \quad \text{ou} \quad \mathbf{a} = \begin{pmatrix}
a_1 \\ 
\vdots \\ 
a_n
\end{pmatrix}.
$$

Quando os elementos estão organizados em linha dizemos que temos um **vetor linha**.
Por outro lado, quando os elementos estão organizados em coluna dizemos que temos um **vetor coluna**.
Um elemento do vetor é chamado de $a_i$, sendo $i$ a posição do elemento no vetor.
O **tamanho de um vetor** é o seu número de elementos. 
 
O **módulo de um vetor** é o seu comprimento 

$$
|\mathbf{a}| = \sqrt{a_1^2 + \ldots + a_n^2}.
$$

O vetor unitário é o vetor com comprimento $1$. Podemos padronizar um vetor qualquer para ter tamanho $1$ dividindo cada elemento do vetor pelo seu comprimento, 

$$
\hat{\mathbf{a}} =  \frac{\mathbf{a}}{|\mathbf{a}|}.
$$

Dizemos que dois vetores são iguais se têm o mesmo tamanho e se todos os seus elementos em posições equivalentes são iguais. 

### Operações com vetores

Da mesma forma que podemos fazer operações soma, subtração, multiplicação e divisão com números (no contexto de Álgebra Linear são chamados de **escalares**) também podemos fazer operações com vetores. No entanto, nem todas as operações usuais de escalares são válidas para vetores. De forma geral, vetores podem ser somados, subtraídos e multiplicados de forma especial. Porém, vetores não podem ser divididos e existem algumas operações especiais com vetores como o produto interno. 
Dois vetores podem ser somados ou subtraídos apenas se forem do mesmo tipo (linha ou coluna) e do mesmo tamanho. Para definirmos as operações com vetores, considere dois vetores $\mathbf{a}$ e $\mathbf{b}$ adequados, ou seja, mesmo tipo e tamanho, e $\alpha$ um escalar, as seguintes operações são bem definidas: 

1. Soma: $\mathbf{a} + \mathbf{b} = (a_i + b_i) = (a_1 + b_1, \ldots, a_n + b_n).$ 
2. Subtração: $\mathbf{a} - \mathbf{b} = (a_i - b_i) = (a_1 - b_1, \ldots, a_n - b_n).$ 
3. Multiplicação por escalar: $\alpha \mathbf{a} = (\alpha a_1, \ldots, \alpha a_n)$. 
4. Transposta de um vetor: a operação transposta transforma um **vetor coluna** em um **vetor linha** e vice-versa. Por exemplo, 

$$
\mathbf{a} = \begin{pmatrix}
a_1 & \ldots & a_n
\end{pmatrix} \quad  \quad \mathbf{a}^{\top} = \begin{pmatrix}
a_1 \\ 
\vdots \\ 
a_n
\end{pmatrix}.
$$ 

5. Produto interno ou escalar entre dois vetores resulta em um escalar:
 
$$
\mathbf{a} \cdot \mathbf{b} = (a_1 b_1 + a_2 b_2 + \ldots + a_n b_n).
$$ 

O co-seno do ângulo $\theta$ entre os vetores $\mathbf{a}$ e $\mathbf{b}$ é dado por: 

$$
\begin{equation}    
\cos(\theta)=\frac{\mathbf{a}^{\top}\mathbf{b}}{\sqrt{\mathbf{a}^{\top}\mathbf{a}}\sqrt{\mathbf{b}^{\top}\mathbf{b}}}.
\end{equation}
$$

Dizemos que dois vetores são **ortogonais** entre si se o ângulo $\theta$ entre eles é 90º o que implica que $\cos(\theta)=0$ e que $\mathbf{a}^{\top} \mathbf{b}=0$. 
Todas as operações descritas para vetores são trivialmente definidas em `R`. Vamos ver alguns exemplos. Para declarar um vetor em `R` usamos a função `c()`. 

``` {r}
a <- c(4,5,6)
b <- c(1,2,3)
```

Sendo os vetores compatíveis podemos facilmente somá-los, subtraí-los, multiplicar por um escalar ou obter o produto interno, conforme ilustrado no Código 2.1. Porém, cabe enfatizar que a divisão entre vetores não é definida. 

**Código 2.1** Operações com vetores. 

``` {r} 
## Soma

a + b
```

``` {r} 
## Subtração

a - b
```

``` {r} 
## Multiplicação por escalar

alpha = 10
alpha * a
```

``` {r} 
## Produto interno

a %*% b
```
Neste ponto é importante falar sobre a **lei da reciclagem** em `R` e dos cuidados que devemos ter ao fazer operações com vetores. 

Vamos definir dois vetores, $a$ e $b$, porém de tamanhos diferentes. 

``` {r}
a <- c(4,5,6,5,6,7)
b <- c(1,2,3)

a + b
```

Note que apesar dos vetores não serem compatíveis para a soma, o `R` realizou alguma operação. É neste ponto que aparece a **lei da reciclagem**. Veja que os três primeiros elementos do vetor resultante da soma de `a + b` são $4 + 1 = 5; 5 + 2 = 7$ e $6 + 3 = 9$. Porém, o `R` ainda reporta mais três números que são o resultado de $5 + 1 = 6; 6 + 2 = 8$ e $7 + 3 = 10$, ou seja, o `R` reciclou os elementos do vetor `b` para completar a tarefa de somar `a + b`. Portanto, é muito importante tomar cuidado ao fazer operações com vetores em `R`. 

Um outro ponto importante é a forma de fazer a multiplicação entre vetores. Veja o uso do operador especial `%*%` para a multiplicação entre vetores. O símbolo usual de multiplicação `*` não realiza a multiplicação vetorial, mas sim uma multiplicação elemento por elemento, também chamada de produto de Hadamard. 

``` {r}
a <- c(4,5,6)
b <- c(1,2,3)
```

``` {r}
## Multiplicação elemento a elemento (Hadamard)

a*b
```

``` {r}
## Multiplicação entre vetores

a%*%b
```

Usando as operações com vetores podemos facilmente calcular o co-seno do ângulo $\theta$ entre dois vetores compatíveis. 

``` {r}
cos_theta <- t(a)%*%b/(sqrt(t(a)%*%a)*sqrt(t(b)%*%b))
cos_theta
```

## Matrizes 

::: {.callout-important appearance="simple"}

## **Definição 2.2**

Uma **matriz** é um arranjo retangular ou quadrado de números ou variáveis.
Uma matriz $(n \times m)$ tem $n$ linhas e $m$ colunas:

$$ 
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & \ldots & a_{1m} \\ 
a_{21} & a_{22} & \ddots & a_{2m} \\ 
\vdots & \vdots & \ddots &  \vdots \\ 
a_{n1} & \ldots & \ldots & a_{nm}
\end{pmatrix}.
$$ 

:::

Em termos de notação, nós vamos usar letras maiúsculas em negrito para representar uma matriz, por exemplo: $\mathbf{A}$. Neste livro os elementos de uma matriz serão números reais ou variáveis representando números reais. Por exemplo, a matriz $\mathbf{A}$ representa as notas e o número de faltas de três alunos do curso de estatística básica. Cada linha representa um aluno, a primeira coluna a nota e a segunda o número de faltas. 

$$
\mathbf{A} = \begin{pmatrix}
80 & 0 \\ 
95 & 1 \\ 
70 & 5
\end{pmatrix}.
$$

Para representar os elementos da matriz como variáveis ou incógnitas, usamos letras minúsculas, por exemplo, 

$$
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} \\ 
a_{21} & a_{22} \\ 
a_{31} & a_{32}
\end{pmatrix}.
$$ 

Vamos adotar que o primeiro subscrito representa linha e o segundo a coluna do elemento, ou seja, $a_{\text{linha}\;\text{coluna}}$. Assim, podemos representar a matriz também por meio dos seus elementos, $\mathbf{A} = a_{ij}$ para $i=1, \ldots, n$ e $j=1, \ldots, m$ onde $n$ e $m$ são o número de linhas e colunas da matriz, respectivamente. Para o exemplo dos alunos, temos $n = 3$ e $m = 2$. É comum referenciar uma matriz pela sua dimensão ou tamanho, ou seja, o número de linhas e colunas. Dizemos, então que $\mathbf{A}$ tem dimensão $n \times m$, leia-se “A matriz $\mathbf{A}$ tem dimensão $n$ por $m$.” Particularizando para o nosso exemplo $\mathbf{A}$ é $3 \times 2$. 
Note que um vetor é simplesmente uma matriz com apenas uma linha ou uma coluna. De forma similar, podemos pensar que um escalar é apenas uma matriz de dimensão $1 \times 1$. No entanto, um escalar é tecnicamente diferente de uma matriz $1 \times 1$ em termos de aplicações e propriedades em Álgebra Linear. Tais diferenças ficarão claras no decorrer do Capítulo. 
Dizemos que duas matrizes são iguais se tem a mesma dimensão e se os elementos das correspondentes posições são iguais. A operação de transpor também é definida para matrizes de forma similar ao realizado para vetores. Assim, a operação de transposição rearranja uma matriz de forma que suas linhas são transformadas em colunas e vice-versa. Considere o exemplo, 

$$ 
\begin{pmatrix}
1 & 2\\ 
3 & 4\\ 
5 & 6
\end{pmatrix}^{\top} = 
\begin{pmatrix}
1 & 3 & 5\\ 
2 & 4 & 6
\end{pmatrix}.
$$

Além disso, é claro que $(\mathbf{A}^{\top})^{\top} = \mathbf{A}$, ou seja, a transposta da transposta de uma matriz é a matriz original. 
Para definir uma matriz em `R` usamos o comando `matrix()` que permite rearranjar um vetor em uma matriz. O Código 2.2 define uma matriz $3 \times 2$ em `R`. 


**Código 2.2** Inicialização de uma matriz. 

``` {r}
a <- c(1,2,3,4,5,6)
A <- matrix(a, nrow = 3, ncol = 2)

A
```

Por  *default * o `R` vai preencher a matriz por coluna. Assim, os primeiros três valores do vetor formam a primeira coluna, enquanto que os três últimos formam a segunda coluna. Os argumentos `nrow` e `ncol` definem o número de linhas e colunas da matriz resultante. É possível também preencher a matriz por linhas, caso seja de interesse. Neste caso usamos o argumento `byrow = TRUE`. 

``` {r}
A <- matrix(a, nrow = 3, ncol = 2, byrow = TRUE)
A
```

A transposição de uma matriz ou vetor é realizada pela função `t()`, conforme ilustrado no Código 2.3. 

**Código 2.3** Transposta de uma matriz. 

``` {r} 

## Transposta de uma matriz

t(A)
```

### Operações com matrizes

Algumas operações entre matrizes e vetores também são definidas.
Qualquer escalar pode ser multiplicado por qualquer matriz da seguinte forma: 

$$ 
\alpha \mathbf{A} = \begin{pmatrix}
\alpha a_{11} & \alpha a_{12} & \ldots & \alpha a_{1m} \\ 
\alpha a_{21} & \alpha a_{22} & \ddots & \alpha a_{2m} \\ 
\vdots & \vdots & \ddots &  \vdots \\ 
\alpha a_{n1} & \ldots & \ldots & \alpha a_{nm}
\end{pmatrix}.
$$ 

Em palavras, multiplicar um escalar por uma matriz é simplesmente multiplicar cada entrada da matriz pelo escalar de interesse. O resultado é uma matriz de mesma dimensão da matriz original. Note ainda que $\alpha \mathbf{A} = \mathbf{A} \alpha$.
O código 2.4 ilustra tal operação em `R`. 


**Código 2.4** Multiplicação de escalar por matriz. 

``` {r}
A <- matrix(c(1,2,3,4,5,6), nrow = 3, ncol = 2)
alpha <- 10
alpha*A
```

Duas matrizes podem ser somadas ou subtraídas somente se tiverem o mesmo tamanho. O resultado da soma ou subtração de duas matrizes $\mathbf{A}$ e $\mathbf{B}$ ambas $(n \times m)$ é uma matriz $\mathbf{C}$ de mesmo tamanho cujos elementos são dados por: 

1. Soma $c_{ij} = a_{ij} + b_{ij}.$
2. Subtração $c_{ij} = a_{ij} - b_{ij}.$

Por exemplo, 

$$ 
\begin{pmatrix}
1 & 2\\ 
3 & 4\\ 
5 & 6
\end{pmatrix} + \begin{pmatrix}
10 & 20\\ 
30 & 40 \\ 
50 & 60
\end{pmatrix} = 
\begin{pmatrix}
11 & 22\\ 
33 & 44\\ 
55 & 66
\end{pmatrix}.
$$ 

O Código 2.5 ilustra a soma de matrizes em `R`. 

**Código 2.5** Soma de matrizes. 

``` {r}
A <- matrix(c(1,2,3,4,5,6), nrow = 3, ncol = 2)
B <- matrix(c(10,20,30,40,50,60), nrow = 3, ncol = 2)
C = A + B

C
```

A multiplicação $\mathbf{C} = \mathbf{A} \mathbf{B}$ é definida apenas quando o número de colunas de $\mathbf{A}$ é igual ao número de linhas de $\mathbf{B}$. $\underset{m \times n}{\mathbf{C}} = \underset{m \times q}{\mathbf{A}} \underset{q \times n}{\mathbf{B}}$. Cada elemento $c_{ij} = \sum_{k = 1}^q a_{ik} b_{kj}.$ 

Por exemplo, 

$$
\begin{pmatrix}
2 & -1\\ 
8 & 3\\ 
6 & 7
\end{pmatrix} \begin{pmatrix}
4 & 9 & 1 & -3\\ 
-5 & 2 & 4 & 6
\end{pmatrix} =
$$ 

$$
\begin{pmatrix}
(2 \cdot 4 + -1 \cdot-5) & (2\cdot 9 + -1 \cdot 2) & (2\cdot1 + -1 \cdot 4) & (2 \cdot -3 + -1 \cdot 6) \\ 
(8\cdot 4 + 3 \cdot -5) & (8\cdot 9 + 3 \cdot 2)  & (8 \cdot 1 + 3 \cdot 4) & (8 \cdot -3 + 3 \cdot 6) \\ 
(6\cdot4 + 7\cdot -5) & (6 \cdot 9 + 7 \cdot 2)  & (6 \cdot 1 + 7 \cdot 4) & (6 \cdot - 3 + 7 \cdot 6)
\end{pmatrix} =
$$

$$
\begin{pmatrix}
13 & 16 & -2 & -12\\ 
17 & 78 & 20 & -6\\ 
-11 & 68 & 34 & 24
\end{pmatrix}.
$$ 

O Código 2.6 ilustra a multiplicação de matrizes em `R`. Novamente note o uso do operador especial `%*%` ao invés do símbolo usual de multiplicação `*`. 

**Código 2.6** Multiplicação de matrizes. 

``` {r}
A <- matrix(c(2,8,6,-1,3,7), nrow = 3, ncol = 2)
B <- matrix(c(4,-5,9,2,1,4,-3,6), nrow = 2, ncol = 4)
C = A%*%B

C
```

Em geral $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$. Veja que no caso das matrizes do exemplo anterior $\mathbf{B}$ tem dimensão $2\times 4$, enquanto $\mathbf{A}$ tem dimensão $3 \times 2$, e portanto o produto não pode ser feito. Apenas como ilustração o Código 2.7 tenta realizar a multiplicação entre duas matrizes não compatíveis. 

**Código 2.7** Multiplicação de matrizes não compatíveis. 

``` {r, error=TRUE}
B %*% A
```

Neste caso o `R` retorna uma mensagem de erro indicando que as matrizes não são compatíveis para multiplicação. 
Algumas propriedades da multiplicação de matrizes são apresentadas a seguir. Sendo $\mathbf{A}, \mathbf{B}, \mathbf{C}$ e $\mathbf{D}$ compatíveis temos, 

1. $\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}$.
2. $(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C}).$
3. $\alpha (\mathbf{A} + \mathbf{B}) = \alpha \mathbf{A} + \alpha \mathbf{B}.$
4. $(\alpha + \beta) \mathbf{A} = \alpha \mathbf{A} + \beta \mathbf{A}.$
5. $\alpha(\mathbf{A}\mathbf{B}) = (\alpha \mathbf{A})\mathbf{B} = \mathbf{A}(\alpha \mathbf{B}).$
6. $\mathbf{A}(\mathbf{B} \pm \mathbf{C}) = \mathbf{A}\mathbf{B} \pm \mathbf{A}\mathbf{C}.$
7. $(\mathbf{A} \pm \mathbf{B})\mathbf{C} = \mathbf{A}\mathbf{C} \pm \mathbf{B}\mathbf{C}.$
8. $(\mathbf{A} - \mathbf{B})(\mathbf{C} - \mathbf{D}) = \mathbf{A}\mathbf{C} - \mathbf{B}\mathbf{C} - \mathbf{A}\mathbf{D} + \mathbf{B}\mathbf{D}.$

Duas propriedades interessantes envolvendo transposta e multiplicação de matrizes são: 

1. Se $\mathbf{A}$ é $n \times m$ e $\mathbf{B}$ é $m \times n$, então

$$
(\mathbf{A} \mathbf{B})^{\top} = \mathbf{B}^{\top} \mathbf{A}^{\top}.
$$

2. De maneira similar, se $\mathbf{A}, \mathbf{B}$ e $\mathbf{C}$ são compatíveis

$$
(\mathbf{A} \mathbf{B} \mathbf{C} )^{\top} = \mathbf{C}^{\top} \mathbf{B}^{\top} \mathbf{A}^{\top}.
$$

Neste momento estamos aptos a fazer a distinção entre escalar e uma matriz $1 \times 1$. Considere que $\mathbf{a}$ é uma matriz $1 \times 1$. O produto de um escalar ($\alpha$) por uma matriz está definido para qualquer matriz. Entretanto, uma matriz $1 \times 1$ só pode ser multiplicada por um vetor $\mathbf{b}$ ($1 \times n$, vetor linha) pela direita, ou seja, $\mathbf{a} \mathbf{b}$ ou pela esquerda por um vetor $\mathbf{b}$ ($n \times 1$, vetor coluna)
pela esquerda $\mathbf{b} \mathbf{a}$. O Código 2.8 ilustra esta situação. 


**Código 2.8** Ilustração da diferença entre escalar e matriz $1 \times 1$. 

``` {r}
alpha <- 10
a <- matrix(10, nrow = 1, ncol = 1)
b <- matrix(c(1,2,3,4), nrow = 1, ncol = 4)

dim(a) # Dimensão de a
```

``` {r}
dim(b) ## Dimensão de b
```

``` {r}
a%*%b # Compatível
```

``` {r, error = TRUE}
b%*%a ## Não compatível
```

``` {r}
t(b)%*%a ## Compatível
```

``` {r}
alpha*b # Escalar com matriz
```

``` {r}
b*alpha # Escalar com matriz
```

Um outro tipo de produto entre vetores ou matrizes é o chamado **produto de Hadamard**. Sendo duas matrizes ou dois vetores de mesmo tamanho o produto de Hadamard é simplesmente o resultado da multiplicação direta dos elementos correspondentes: 

$$
\mathbf{A} \odot  \mathbf{B} = \begin{pmatrix}
a_{11} b_{11} & a_{12} b_{12} & \cdots & a_{1m}b_{1m} \\ 
a_{21} b_{21} & a_{22} b_{22} & \cdots & a_{2m} b_{2m} \\ 
\vdots & \vdots & \vdots & \vdots\\ 
a_{n1} b_{n1} & a_{n2} b_{n2} & \cdots & a_{nm} b_{nm}
\end{pmatrix}.
$$

Estamos usando a notação $\odot$ para diferenciar o produto de Hadamard do produto matricial usual. Em `R` este produto é obtido usando o operador `*` usual, conforme ilustrado no Código 2.9. 

**Código 2.9** Produto de Hadamard. 

``` {r}
A <- matrix(c(1,2,3,4), nrow = 2, ncol = 2)
B <- matrix(c(10,20,30,40), nrow = 2, ncol = 2)

A*B
```

### Matrizes de formas especiais

Nesta subseção veremos algumas matrizes com formas especiais. 

Dizemos que uma matriz é quadrada quando tem o mesmo número de linhas e colunas. Por exemplo, 

$$
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\ 
a_{21} & a_{22} & a_{23} & a_{24} \\ 
a_{31} & a_{32} & a_{33} & a_{34} \\ 
a_{41} & a_{42} & a_{43} & a_{44} 
\end{pmatrix}.
$$ 

Chamamos os elementos $a_{ii}$ de elementos **diagonais** ou da diagonal. Já os elementos $a_{ij}$ para $i \neq j$ são chamados de elementos **fora da diagonal**. Os elementos $a_{ij}$ para $j > i$ são os elementos **acima da diagonal** e os elementos $a_{ij}$ para $i > j$ são os elementos **abaixo da diagonal**. 

Uma matriz é diagonal se apenas os elementos diagonais são diferentes de zero. Por exemplo, 

$$
\mathbf{D} = \begin{pmatrix}
a_{11} & 0 & 0 & 0 \\ 
0 & a_{22} & 0 & 0 \\ 
0 & 0 & a_{33} & 0 \\ 
0 & 0 & 0 & a_{44} 
\end{pmatrix}.
$$ 

Uma matriz é triangular superior se os elementos abaixo da diagonal são todos iguais a zero. É comum denotar uma matriz triangular superior por $\mathbf{U}$ do Inglês  *upper *. 

$$
\mathbf{U} = \begin{pmatrix}
a_{11} & a_{12} & a_{13} & a_{14} \\ 
0 & a_{22} & a_{23} & a_{24} \\ 
0 & 0 & a_{33} & a_{34} \\ 
0 & 0 & 0 & a_{44} 
\end{pmatrix}.
$$ 

Por outro lado, uma matriz é triangular inferior se os elementos acima da diagonal são todos iguais a zero. Neste caso a notação usual é $\mathbf{L}$ do Inglês  *lower *. 

$$
\mathbf{L} = \begin{pmatrix}
a_{11} & 0 & 0 & 0 \\ 
a_{21} & a_{22} & 0 & 0 \\ 
a_{31} & a_{32} & a_{33} & 0 \\ 
a_{41} & a_{42} & a_{43} & a_{44} 
\end{pmatrix}.
$$ 

A matriz identidade é uma matriz diagonal onde os elementos diagonais são todos iguais a $1$. A notação usual é $\mathbf{I}$ do Inglês  *identity *. 

$$
\mathbf{I} = \begin{pmatrix}
1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 \\ 
0 & 0 & 1 & 0 \\ 
0 & 0 & 0 & 1
\end{pmatrix}.
$$ 

A matriz zero ou nula é aquela cuja todas as entradas são iguais a zero, ou seja, 

$$
\mathbf{0} = \begin{pmatrix}
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0
\end{pmatrix}.
$$ 

Uma matriz quadrado é dita ser simétrica se $a_{ij} = a_{ji}$. De forma equivalente, se $\mathbf{A}^{\top} = \mathbf{A}$, então $\mathbf{A}$ é simétrica. Por exemplo, 

$$
\mathbf{A} = \begin{pmatrix}
1 & 0.8 & 0.6 & 0.4 \\ 
0.8 & 1 & 0.2 & 0.4 \\ 
0.6 & 0.2 & 1 & 0.1 \\ 
0.4 & 0.4 & 0.1 & 1
\end{pmatrix}.
$$ 

### *Rank * e inversa de uma matriz

Na seção 2.2.1 vimos diversas operações com matrizes. Muitas destas operações são extensões das operações de soma, subtração e multiplicação de escalares. No entanto, a divisão entre matrizes não foi definida. De forma geral duas matrizes não podem ser divididas, porém existe um tipo especial de matriz que tenta de certa forma estender a operação de divisão entre escalares para matrizes. Essa matriz especial é chamada de **matriz inversa**. Porém, a sua obtenção está limitada a um certo conjunto de matrizes que são chamadas **não singulares**. Nesta seção nós vamos entender o que é uma matriz não singular e como avaliar se uma matriz qualquer é ou não singular por meio da avaliação do seu  *rank *, também chamado de **posto**. 

Para definir o que é o  *rank * de uma matriz precisamos da noção de **dependência** e **independência linear**. Um conjunto de vetores $\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n$ é dito ser **linearmente dependente** se escalares $c_1, c_2, \ldots, c_n$ não todos iguais a zero puderem ser encontrados de tal forma que 

$$
\begin{equation}
c_1 \mathbf{a}_1 + c_2 \mathbf{a}_2 + \ldots + c_n \mathbf{a}_n = 0.
\tag{2.1}
\end{equation}
$$ 

No caso em que os coeficientes $c_1, c_2, \ldots, c_n$ não puderem ser encontrados satisfazendo (2.1) o conjunto de vetores $\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n$ é dito ser **linearmente independente**. Note que a Equação (2.1) pode ser reescrita em formato matricial, onde os vetores $\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n$ formam as colunas de uma matriz $\mathbf{A}$ e os escalares são empilhados em um vetor $\mathbf{c}$. Assim, 

$$
\mathbf{A} \mathbf{c} = \mathbf{0}.
$$

Neste caso as colunas de $\mathbf{A}$ são linearmente independentes se $\mathbf{A} \mathbf{c} = \mathbf{0}$ implicar que $\mathbf{c} = 0$. Intuitivamente um conjunto de vetores linearmente dependentes é de alguma forma redundante, no sentido de que um dos vetores pode ser escrito como uma combinação linear dos outros. 

Considere um exemplo trivial com os vetores $\mathbf{a}_1 = (1,0)$ e $\mathbf{a}_2 = (0,1)$. Veja que qualquer outro vetor de tamanho dois pode ser escrito como uma combinação linear dos vetores $\mathbf{a}_1$ e $\mathbf{a}_2$. Assim, qualquer outro vetor de tamanho dois concatenado com $\mathbf{a}_1$ e $\mathbf{a}_2$ em uma matriz $\mathbf{A}$ tornará as colunas de $\mathbf{A}$ linearmente dependentes. Porém, uma matriz $\mathbf{A}$ formada apenas por $\mathbf{a}_1$ e $\mathbf{a}_2$ tem colunas linearmente independentes. 

::: {.callout-important appearance="simple"}

## **Definição 2.3** 

O  *rank * ou **posto** de qualquer matriz quadrada ou retangular $\mathbf{A}$ é definido como 

$$
\mathrm{rank}(\mathbf{A}) = \text{número de colunas ou linhas linearmente independentes em } \mathbf{A}.
$$ 
:::

Pode ser provado que o número de linhas e colunas linearmente independentes em uma matriz qualquer é sempre o mesmo. Se uma matriz $\mathbf{A}$ tem apenas um elemento diferente de zero, então $\mathrm{rank}(\mathbf{A}) = 1$. O $\mathrm{rank}$ da matriz nula é $0$. 

Sendo $\mathbf{A}$ uma matriz retangular $n \times m$ o maior  *rank * possível para $\mathbf{A}$ é o $\min(n,m)$. Quando o  *rank * da matriz é o $\min(n,m)$ dizemos que a matriz tem  *rank * completo ou é de posto completo. Importante salientar que qualquer matriz retangular terá colunas linearmente dependentes. Com os conceitos apresentados até aqui podemos finalmente definir o que entendemos por uma **matriz não singular** e **matriz inversa**. 

::: {.callout-important appearance="simple"}

## **Definição 2.4**

Uma matriz quadrada de **posto completo** é chamada de **não singular**.
:::

::: {.callout-important appearance="simple"}

## **Definição 2.5**

Dada uma matriz quadrada $\mathbf{A}$ de posto completo a **matriz inversa** de $\mathbf{A}$ denotada por $\mathbf{A}^{-1}$ é única tal que 
 
$$
\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}.
$$ 
:::

Se a matriz $\mathbf{A}$ não for quadrada e de posto completo, então $\mathbf{A}$ não terá inversa e é dita ser **singular**. Baseado na Definição 2.5 é fácil ver que $(\mathbf{A}^{-1})^{-1} = \mathbf{A}.$ 

Em aplicações reais obter a inversa de uma matriz é uma tarefa complexa e requer o uso de algoritmos numéricos. Nós vamos ver algumas opções na Seção 2.3 quando discutiremos algoritmos para a solução de sistemas de equações lineares. 
 
A intuição da matriz inversa é poder fazer operações similares a que realizamos com escalares quando estamos resolvendo sistemas de equações lineares. Lembre-se do sistema de equações lineares representado nas Equações (1.14) e (1.15) no modelo de regressão linear simples. 
 
No decorrer deste livro seremos frequentemente confrontados com sistemas lineares do tipo $\mathbf{A} \mathbf{x} = \mathbf{c}$ em que precisamos encontrar o vetor de incógnitas $\mathbf{x}$. No caso em que $\mathbf{A}$ é não singular, o sistema de equações $\mathbf{A} \mathbf{x} = \mathbf{c}$ terá uma única solução dada por $\mathbf{x} = \mathbf{A}^{-1} \mathbf{c}$. Veja como a inversa imita o que costumamos fazer com escalares quando resolvendo uma simples equação linear. É o equivalente ao que falamos grosseiramente de “passa pro outro lado.” O que realmente está sendo feito é que multiplicamos o sistema em ambos os lados por $\mathbf{A}^{-1}$ para obter a solução, ou seja, 
 
$$
\begin{eqnarray}
\mathbf{A}^{-1} \mathbf{A} \mathbf{x} &=& \mathbf{A}^{-1} \mathbf{c} \\
\mathbf{I}\mathbf{x} &=& \mathbf{A}^{-1} \mathbf{c}.
\end{eqnarray}
$$ 
 
Em `R` podemos facilmente obter a inversa de uma matriz usando a função `solve()`. No entanto, como ficará claro quando discutirmos métodos para a solução de sistemas lineares, tal solução é cara computacionalmente e raramente necessária explicitamente. O Código 2.10 ilustra a obtenção da inversa. 

**Código 2.10** Inversa de uma matriz. 

``` {r}
A <- matrix(c(4, 2, 7, 6), 2, 2)
A_inv <- solve(A)

## Conferindo: deve resultar na matriz identidade
A%*%A_inv
```

Duas propriedades importantes envolvendo multiplicação e inversão de matrizes são: 

1. Se $\mathbf{A}$ é não singular, então $\mathbf{A}^{\top}$ é não singular e sua inversa é dada por
$$
(\mathbf{A}^{\top})^{-1} = (\mathbf{A}^{-1})^{\top}.
$$

2. Se $\mathbf{A}$ e $\mathbf{B}$ são matrizes não singulares de mesmo tamanho, então o produto $\mathbf{A} \mathbf{B}$ é não singular e
$$
(\mathbf{A} \mathbf{B})^{-1} =  \mathbf{B}^{-1} \mathbf{A}^{-1}.
$$

No caso em que uma matriz é retangular não podemos obter a inversa. Nestes casos podemos recorrer a matriz **inversa generalizada**. 

::: {.callout-important appearance="simple"}

## **Definição 2.6** 

A **inversa generalizada** de uma matriz $\mathbf{A}$ $n \times p$ é qualquer matriz $\mathbf{A}^{-}$ que satisfaça 
 
$$
\mathbf{A}\mathbf{A}^{-}\mathbf{A} = \mathbf{A}.
$$ 
:::
 
A inversa generalizada não é única exceto quando $\mathbf{A}$ é não-singular, e neste caso coincide com a inversa. Toda matriz, seja quadrada ou retangular tem uma inversa generalizada, isto inclui vetores. Como um exemplo ilustrativo, considere o vetor 
 
$$
\mathbf{a} = \begin{pmatrix}
1\\ 
2\\ 
3\\ 
4
\end{pmatrix}.
$$ 
 
Neste caso o vetor $\mathbf{a}^{-} = (1, 0, 0, 0)$ é a inversa generalizada de $\mathbf{a}$.
Para verificar basta fazer a multiplicação matricial. 

``` {r}
a <- matrix(c(1, 2, 3, 4), 4, 1)
a_invg <- matrix(c(1,0,0,0), 1, 4)
a%*%a_invg%*%a
```

Importante notar que se $\mathbf{A}$ é $n \times p$, então qualquer inversa generalizada de $\mathbf{A}$ terá dimensão $p \times n$. 
 
Como dito, a inversa generalizada não é única. Em particular a inversa generalizada chamada de Moore-Penrose (*Moore-Penrose Genereralized Inverse *) está implementada em `R` por meio do pacote `MASS`. O Código 2.11 ilustra a obtenção da inversa generalizada de Moore-Penrose para uma matriz quadrada, porém singular. 

**Código 2.11** Inversa generalizada de Moore-Penrose. 

``` {r}
## Matriz singular (a terceira coluna é a soma da primeira com a segunda)
A <- matrix(c(2, 1, 3, 2, 0, 2, 3, 1, 4), 3, 3)

## Carregando o pacote MASS
library(MASS)
A_ginv <- ginv(A)

## Conferindo
A%*%A_ginv%*%A
```

### Matrizes positivas definidas
 
No decorrer deste livro vamos encontrar frequentemente as chamadas somas de quadrados. Nós já discutimos sobre soma de quadrados nas seções 1.2.5 e 1.2.9 quando encontramos a média e os coeficientes de regressão do modelo de regressão linear simples. De forma matricial, a soma de quadrados pode ser reescritas usando **formas quadráticas**. 
 
Considere uma matriz $\mathbf{A}$ simétrica e $\mathbf{y}$ um vetor, o produto 
 
$$
\mathbf{y}^{\top} \mathbf{A} \mathbf{y} = \sum_{i} a_{ij} y_i^2 + \sum_{i \neq j} a_{ij} y_i y_j,
$$

é chamado de **forma quadrática**. 
 
Para uma matriz $\mathbf{y}$ de dimensão $n \times 1$, ou seja, um vetor coluna o produto matricial $\mathbf{y}^{\top}\mathbf{I} \mathbf{y} = y_1^2 + y_2^2 + \ldots, y_n^2$. Assim, $\mathbf{y}^{\top} \mathbf{y}$ é a soma de quadrados dos elementos do vetor $\mathbf{y}$. A raiz quadrado da soma de quadrados é a distância do ponto $\mathbf{y}$ até a origem, também chamada de comprimento de $\mathbf{y}$. As somas de quadrados permanecem positivas, ou ao menos não negativas para todos os valores de $\mathbf{y}$ exceto no caso em que $\mathbf{y} = 0$. 

::: {.callout-important appearance="simple"}

## **Definição 2.7** 

Sendo $\mathbf{A}$ uma matriz simétrica com a propriedade $\mathbf{y}^{\top} \mathbf{A} \mathbf{y} > 0$ para todos os possíveis $\mathbf{y}$ exceto para quando $\mathbf{y} = 0$, então a forma quadrática $\mathbf{y}^{\top} \mathbf{A} \mathbf{y}$ é chamada **positiva definida**, e $\mathbf{A}$ é dita ser uma **matriz positiva definida**. No caso de $\mathbf{y}^{\top} \mathbf{A} \mathbf{y} \geq 0$ e existir ao menos um $\mathbf{y} \neq 0$ tal que $\mathbf{y}^{\top} \mathbf{A} \mathbf{y} = 0$, então $\mathbf{y}^{\top} \mathbf{A} \mathbf{y}$ é dita ser **positiva semi-definida** e $\mathbf{A}$ é uma matriz **positiva semi-definida**. 
:::
 
Para ilustração considere a seguinte matriz 
 
$$
\mathbf{A} = \begin{pmatrix}
2 & -1\\ 
-1 & 3
\end{pmatrix}.
$$

A forma quadrática associada é dada por 
 
$$
\mathbf{y}^{\top} \mathbf{A} \mathbf{y} = \begin{pmatrix}
y_1 & y_2
\end{pmatrix}
\begin{pmatrix}
2 & -1\\ 
-1 & 3
\end{pmatrix} 
\begin{pmatrix}
y_1 \\ 
y_2
\end{pmatrix} = 2 y_1^2 - 2 y_1 y_2 + 3 y_2^2,
$$

que é claramente positiva, desde que $y_1$ e $y_2$ sejam diferentes de zero. Algumas propriedades interessantes sobre matrizes positivas definidas ou semi-definidas são: 

1. $\mathbf{A}$ é positiva definida, então todos os valores da diagonal de $\mathbf{A}$ são positivos.
2. Se $\mathbf{A}$ é positiva semi-definida, então os elementos da diagonal de $\mathbf{A}$ são maiores ou iguais a zero.
3. Sendo $\mathbf{P}$ uma matriz não-singular e $\mathbf{A}$ uma matriz positiva definida, o produto $\mathbf{P}^{\top} \mathbf{A} \mathbf{P}$ é uma matriz positiva definida.
4. Sendo $\mathbf{P}$ uma matriz não-singular e $\mathbf{A}$ uma matriz positiva semi-definida, o produto $\mathbf{P}^{\top} \mathbf{A} \mathbf{P}$ é uma matriz positiva semi-definida.
5. Uma matriz positiva definida é não-singular.

Matrizes positivas definidas são fundamentais em estatística pois representam a matriz de variância e covariância de vetores aleatórios. 

### Determinante e traço de uma matriz

::: {.callout-important appearance="simple"}

## **Definição 2.8** 

O **determinante** de uma matriz $\mathbf{A}$ é o escalar

$$
|\mathbf{A}| = \sum_j (-1)^k a_{1 j_1} a_{2 j_2}, \ldots, a_{n j_n},
$$

onde a soma é realizada para todas as $n!$ permutações de grau $n$, e $k$ é o número de mudanças necessárias para que os segundos subscritos sejam colocados na ordem $1,2, \ldots, n.$ 
:::
 
Em termos de notação vamos usar $|\mathbf{A}|$ ou $\det{(A)}$. A Definição 2.8 é difícil de entender e pouco útil para avaliar determinantes de matrizes de grande dimensão.
Entretanto, nestes casos temos sempre que recorrer a métodos computacionais. Para matrizes pequenas é fácil obter o determinante. Vamos fazer um exemplo simples com uma matriz $2 \times 2$ apenas como ilustração. 
 
Considere a matriz 
 
$$
\mathbf{A} = \begin{pmatrix}
3 & -2\\ 
-2 & 4
\end{pmatrix}.
$$ 
 
Usando a definição precisamos encontrar todas as $n!$ combinações de produtos contendo um elemento de cada linha e coluna. Neste caso $n = 2$ e $n! = 2 \cdot 1 = 2$. Teremos apenas duas combinações. Na primeira combinação vamos pegar o elemento $a_{11} = 3$ e multiplicar pelo elemento $a_{22} = 4$. Neste caso os índices de coluna já estão na ordem crescente, então o número de trocas foi $0$. O segundo termo será o elemento $a_{12} = -2$ multiplicado pelo elemento $a_{21} = -2$. Neste caso, para que os índices de coluna fiquem em ordem crescente, precisamos fazer uma mudança, então $k = 1$. 
 
$$
|\mathbf{A}| = (-1)^0 a_{11} a_{22} + (-1)^1 a_{12} a_{21} = 1 \cdot (3 \cdot 4) - (-2 \cdot -2) =  12 - 4 = 8.
$$ 
 
Em `R` o determinante de uma matriz pode ser obtido por meio da função `determinant()`, conforme ilustrado no Código 2.12. 

**Código 2.12** Determinante de uma matriz. 

``` {r}
A <- matrix(c(3,-2,-2,4),2,2)
determinant(A, logarithm = FALSE)
```
É muito comum precisarmos do logaritmo do determinante, assim a função `determinant()` traz essa opção como  *default *. Por isso, para obter o determinante de $\mathbf{A}$ precisamos incluir o argumento `logarithm = FALSE`. Caso contrário o determinante seria obtido em escala logarítmica. 

``` {r}
determinant(A, logarithm = TRUE)
```

Alguns aspectos interessantes sobre determinantes são: 

1. Se $\mathbf{A}$ é singular, $|\mathbf{A}| = 0$.
2. Se $\mathbf{A}$ é não singular, $|\mathbf{A}| \neq 0$.
3. Se $\mathbf{A}$ é positiva definida, $|\mathbf{A}| > 0$.
4. $|\mathbf{A}^{\top}| = |\mathbf{A}|$.
5. Se $\mathbf{A}$ é não singular, $|\mathbf{A}^{-1}| = \frac{1}{|\mathbf{A}|}$.

::: {.callout-important appearance="simple"}
## **Definição 2.9** 

O **traço** de uma matriz $\mathbf{A}$ $n \times n$ é um escalar definido como a soma dos elementos da diagonal, $\mathrm{tr}(\mathbf{A}) = \sum_{i=1}^n a_{ii}$. 
:::
 
Algumas propriedades do traço: 

1. Se $\mathbf{A}$ e $\mathbf{B}$ são $n \times n$, então

$$
\mathrm{tr}(\mathbf{A} + \mathbf{B}) = \mathrm{tr}(\mathbf{A}) + \mathrm{tr}(\mathbf{B}).
$$

2. Se $\mathbf{A}$ é $n \times p$ e $\mathbf{B}$ e $p \times n$, então

$$
\mathrm{tr}(\mathbf{AB}) = \mathrm{tr}(\mathbf{BA}).
$$

Em `R` não temos uma função explícita para a obtenção do traço devido a sua simplicidade. Assim, podemos obter o traço simplesmente acessando os elementos da diagonal da matriz com a função `diag()` e somar com a função `sum()`, conforme ilustrado no Código 2.13. 

**Código 2.13** Traço de uma matriz.

``` {r}
A <- matrix(c(3,-2,-2,4),2,2)
sum(diag(A))
```

### Cálculo vetorial e matricial
 
No Capítulo $1$ vimos como obter a derivada de funções com até duas variáveis independentes. Usando as ferramentas de Cálculo vetorial e matricial podemos obter derivadas de funções com um número arbitrário de variáveis independentes. Seja $y = f(\mathbf{x})$ uma função das variáveis $x_1, x_2, \ldots, x_p$ e $\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \ldots, \frac{\partial y}{\partial x_p}$ as respectivas derivadas parciais. Assim, 
 
$$
\frac{\partial y}{\partial \mathbf{x}} = \begin{pmatrix}
\frac{\partial y}{\partial x_1}\\ 
\frac{\partial y}{\partial x_2}\\ 
\vdots\\ 
\frac{\partial y}{\partial x_p}
\end{pmatrix}.
$$ 
 
Basicamente, a ideia é derivar em cada uma das variáveis independentes e arranjar as derivadas parciais em um vetor de tamanho adequado. Usando esta ideia simples é possível calcular a derivada de funções mais complicadas. Vejamos algumas derivadas vetoriais e matriciais úteis em ciência de dados. 
 
Sendo $\mathbf{a}^{\top} = (a_1, a_2, \ldots, a_p)$ um vetor de constantes e $\mathbf{A}$ uma matriz simétrica de constantes. 

1. Seja $y = \mathbf{a}^{\top} \mathbf{x} = \mathbf{x}^{\top} \mathbf{a}$. Então,

$$
\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial (\mathbf{x}^{\top} \mathbf{a})}{\partial \mathbf{x}} = \mathbf{a}.
$$ 

2. Seja $y = \mathbf{x}^{\top} \mathbf{A} \mathbf{x}$. Então,

$$
\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial (\mathbf{x}^{\top} \mathbf{A} \mathbf{x}) }{\partial \mathbf{x}} = 2 \mathbf{A} \mathbf{x}.
$$ 
 
De forma similar, se $y = f(\mathbf{X})$ onde $\mathbf{X}$ é uma matriz $p \times p$.
As derivadas parciais de $y$ em relação a cada $x_{ij}$ são organizadas em uma matriz. 
 
$$
\frac{\partial y}{\partial \mathbf{X}} = \begin{pmatrix}
\frac{\partial y}{\partial x_{11}} & \ldots & \frac{\partial y}{\partial x_{1p}}\\ 
\vdots & \ddots  & \vdots\\ 
\frac{\partial y}{\partial x_{p1}} & \ldots & \frac{\partial y}{\partial x_{pp}} 
\end{pmatrix}.
$$ 
 
Algumas derivadas importantes envolvendo matrizes são apresentadas abaixo. 

1. Seja $y = \mathrm{tr}(\mathbf{X}\mathbf{A})$ sendo $\mathbf{X}$ $p \times p$ e definida positiva e $\mathbf{A}$ $p \times p$ constantes. Então,

$$
\frac{\partial y}{\partial \mathbf{X}} = \frac{\partial \mathrm{tr}(\mathbf{X}\mathbf{A})}{\partial \mathbf{X}} =  \mathbf{A} + \mathbf{A}^{\top} - \mathrm{diag}(\mathbf{A}).
$$ 

2. Sendo $\mathbf{A}$ não singular com derivadas $\frac{\partial \mathbf{A}}{\partial x}$. Então,

$$
\frac{\partial \mathbf{A}^{-1}}{\partial x} = - \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x} \mathbf{A}^{-1}.
$$

3. Sendo $\mathbf{A}$ $n \times n$ positiva definida. Então,

$$
\frac{\partial \log |\mathbf{A}|}{\partial x} = \mathrm{tr} \left( \mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial x} \right).
$$ 


### Regressão linear múltipla
 
Na seção 1.2.9 construímos o modelo de regressão linear simples, onde apenas uma covariável $x$ descreve o comportamento da variável dependente $y$, por meio de uma reta, ou seja, 
 
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$ 
 
No modelo de regressão linear múltipla estendemos este modelo para levar em consideração um número arbitrário $p$ de covariáveis $x_{ip}$. Nesta notação $x_{ip}$ é o valor da $p-$ésima covariável associada a observação $i$ para $i = 1, \ldots, n$, sendo $n$ o número de observações. Note que a primeira covariável é assumida como $1$ para representar o intercepto. 
 
O modelo de regressão linear múltipla é então escrito como 
 
$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i.
$$

Podemos escrever o modelo para cada uma das $n$ observações, como segue 
 
$$
\begin{matrix}
y_1 = \beta_0 + \beta_1 x_{11} + \beta_2 x_{12} + \ldots \beta_{p} x_{1p} + \epsilon_1 \\ 
y_2 = \beta_0 + \beta_1 x_{21} + \beta_2 x_{22} + \ldots \beta_{p} x_{2p} + \epsilon_2\\ 
\vdots \\ 
y_n = \beta_0 + \beta_1 x_{n1} + \beta_2 x_{n2} + \ldots \beta_{p} x_{np} + \epsilon_p.\\ 
\end{matrix}
$$

Agora organizamos os termos em formato matricial 
 
$$
\underset{n\times 1}{\begin{bmatrix}
y_1\\ 
y_2\\ 
\vdots \\ 
y_n
\end{bmatrix}} = 
\underset{n\times p}{\begin{bmatrix}
1 & x_{11} & \ldots & x_{p1} \\ 
1 & x_{12} & \ldots & x_{p1} \\ 
\vdots & \vdots  & \ddots & \vdots \\ 
1& x_{1n} & \ldots & x_{pn}
\end{bmatrix} }
\underset{p \times 1}{
\begin{bmatrix}
\beta_0 \\ 
\vdots \\ 
\beta_p
\end{bmatrix}
} + \underset{n\times 1}{\begin{bmatrix}
\epsilon_1\\ 
\epsilon_2\\ 
\vdots \\ 
\epsilon_n
\end{bmatrix}}
$$ 
 
Por fim, usamos uma notação mais compacta para representar o modelo 
 
$$ 
\underset{n \times 1}{\mathbf{y}} = \underset{n \times p }{\mathbf{X}} \underset{p\times 1}{\boldsymbol{\beta}} + \underset{n \times 1}{\mathbf{\epsilon}}.
$$ 
 
Note como todas as multiplicações envolvidas no modelo são compatíveis respeitando as regras dos produtos de matrizes. 
 
Nosso objetivo é encontrar o vetor $\boldsymbol{\hat{\beta}}$, tal que 
 
$$
SQ(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^{\top} (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}),
$$

seja a menor possível. 
 
Esse processo é o que chamamos de estimação dos parâmetros de regressão ou de treinamento do modelo, a segunda nomenclatura é comum na literatura de aprendizado de máquina. Note que novamente temos um processo de minimização, porém agora de uma função com muitas variáveis independentes. Usando as ferramentas de Álgebra Linear podemos facilmente proceder com esse problema de minimização. 
 
O primeiro passo é derivar a soma de quadrados em $\boldsymbol{\beta}$, usando Cálculo vetorial (ver Seção 2.2.6). 
 
Derivando em $\boldsymbol{\beta}$, temos

$$
\begin{eqnarray*}
\frac{\partial SQ(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &=& \frac{\partial}{\partial \boldsymbol{\beta}} (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^{\top} (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\
&=& \frac{\partial}{\partial \boldsymbol{\beta}} \left ( (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^{\top}  \right ) (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^{\top} \frac{\partial}{\partial \boldsymbol{\beta}}  (\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\
&=& -\mathbf{X}^{\top}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^{\top} (-\mathbf{X}) \\
&=& -2\mathbf{X}^{\top}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}).
\end{eqnarray*}
$$ 
 
O segundo passo é resolver o sistema de equações lineares resultantes 
 
$$
\begin{eqnarray}
\mathbf{X}^{\top}(\mathbf{y} - \mathbf{X}\boldsymbol{\hat{\beta}}) &=& \boldsymbol{0} \\
\mathbf{X}^{\top}\mathbf{y} - \mathbf{X}^{\top}\mathbf{X}\boldsymbol{\hat{\beta}} &=& 0 \\
\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\hat{\beta}} &=& \mathbf{X}^{\top}\mathbf{y} \tag{2.2} \\ 
\boldsymbol{\hat{\beta}} &=& (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}. \tag{2.3}
\end{eqnarray}
$$ 
 
Note que a Equação (2.2) foi multiplicada em ambos os lados por $(\mathbf{X}^{\top}\mathbf{X})^{-1}$ para encontrar (2.3).
Essa operação corresponde a resolver o sistema de equações lineares.
Nós vimos na Seção 2.2.3 como obter a inversa de uma matriz usando o `R`.
Assim, estamos aptos a implementar tais operações em `R`. 
 
Para exemplificar a implementação do modelo de regressão linear múltipla, vamos usar um conjunto de dados muito famoso sobre o preço de imóveis na cidade de Boston. O conjunto de dados está disponível no pacote `MASS` e contém além da variável resposta $13$ covariáveis e $506$ observações. As primeiras seis observações do conjunto de dados são apresentadas abaixo. 

``` {r}
require(MASS)
data(Boston)
head(Boston)
```
As covariáveis disponíveis são 

* crim: taxa de crimes per capita.
* zn: proporção de terrenos residenciais zoneados para lotes com mais de 25.000 pés quadrados.
* indus: proporção de acres de negócios não varejistas por cidade.
* chas: variável dummy de Charles River (1 se a área limita o rio; 0 caso contrário).
* nox: concentração de óxido de nitrogênio (parte por 10 milhões).
* rm: número médio de quartos por habitação.
* age: proporção de unidades ocupadas pelo proprietário construídas antes de 1940.
* dis: média ponderada das distâncias a cinco centros de empregos de Boston.
* rad: índice de acessibilidade às rodovias radiais.
* tax: taxa de imposto sobre a propriedade de valor total por \$10.000.
* ptratio: proporção aluno-professor por cidade.
* black: $1000 (Bk - 0,63)^2$ onde $Bk$ é a proporção de negros por cidade.
* lstat: Porcentagem da população em pobreza.
 
A variável resposta é o valor mediano das casas ocupadas pelos proprietários em $\$1000$, codificada como `medv`. O primeiro passo para implementar o modelo de regressão linear múltipla para este problema é montar a matriz $\mathbf{X}$, também chamada de matriz de delineamento. Ela vai conter os valores de todas as covariáveis de interesse. Em `R` podemos usar a função `model.matrix()` para construir tal matriz. Para este exemplo, vamos usar apenas as primeiras cinco covariáveis. 

``` {r}
X <- model.matrix(~ crim + zn + indus + chas + nox, data = Boston)
head(X)
```

Note que a função `model.matrix()` automaticamente inclui uma coluna de $1$’s para representar o intercepto. O vetor $y$ neste caso é a coluna `medv`. 

``` {r}
y <- Boston$medv
```
 
Por fim, implementamos a Equação (2.3) de duas formas distintas. Convido o leitor a tentar entender qual a diferença computacional entre elas. Tal diferença ficará clara quando discutirmos algoritmos para a solução de sistemas de equações lineares e obtenção da matriz inversa. 

``` {r}
# Forma ingênua
solve(t(X)%*%X)%*%t(X)%*%y
```

``` {r}
# Forma eficiente
solve(t(X)%*%X, t(X)%*%y)
```

Podemos usar a função `lm()` para conferir a nossa implementação. 

``` {r}
coef(lm(medv ~ crim + zn + indus + chas + nox, data = Boston))
```

Os resultados são idênticos. Podemos usar o modelo para predizer o valor de um imóvel dado os valores de suas covariáveis. Basicamente, tudo o que foi discutido no caso do modelo de regressão linear simples se adapta naturalmente para o modelo de regressão linear múltipla. 
